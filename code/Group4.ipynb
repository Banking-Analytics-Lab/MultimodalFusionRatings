{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aed32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_main=pd.read_csv(\"data.csv\")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "# Set multiple gpus\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "\n",
    "#change of the rating classes to new classes\n",
    "import numpy as np\n",
    "\n",
    "lbl_first=data_main[\"RATING\"].values\n",
    "lbl_final=lbl_first.copy()\n",
    "\n",
    "dict_trans_calss={1:[1,2,3,4,5],\n",
    "                 10:[10,11,12],\n",
    "                  13:[13,14,15],\n",
    "                  16:[16,17,18,19,20,21,22]\n",
    "                  }\n",
    "\n",
    "\n",
    "for calss_main_idx in dict_trans_calss:\n",
    "  lst_class=dict_trans_calss[calss_main_idx]\n",
    "  for idx, item in enumerate(lst_class):\n",
    "    lbl_final = np.where(lbl_first == item, calss_main_idx, lbl_final)\n",
    "    \n",
    "lbl_final_rest=lbl_final.copy()\n",
    "uni_class=np.unique(lbl_final)\n",
    "for idx,item in enumerate(uni_class):\n",
    "    lbl_final_rest=np.where(lbl_final == item, idx, lbl_final_rest)\n",
    "unique, counts = np.unique(lbl_final_rest, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "data_pre=data_main.copy()\n",
    "data_token=data_pre[\"cleantext\"].values\n",
    "##########################Preprocessing\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from cleantext import clean\n",
    "\n",
    "# get lemmatized review\n",
    "def get_lemmatized_text(corpus):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "# get stemmed review\n",
    "def get_stemmed_text(corpus):  \n",
    "    stemmer = PorterStemmer()\n",
    "    out_text=[' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n",
    "    return out_text\n",
    "\n",
    "\n",
    "# remove all stopwords in english review\n",
    "def remove_stop_words(corpus):\n",
    "    english_stop_words = stopwords.words('english')\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split() \n",
    "                      if word not in english_stop_words])\n",
    "        )\n",
    "    return removed_stop_words\n",
    "\n",
    "# preprocess review\n",
    "def preprocess_txt(reviews):\n",
    "    REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    NO_SPACE = \"\"\n",
    "    SPACE = \" \"\n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, reviews.lower())]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    return reviews\n",
    "# clean review by cleantext libaray\n",
    "def clean_text(sent):\n",
    "  clean_sent=clean(sent,\n",
    "      fix_unicode=True,               # fix various unicode errors\n",
    "      to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "      lower=True,                     # lowercase text\n",
    "      no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
    "      no_urls=True,                  # replace all URLs with a special token\n",
    "      no_emails=True,                # replace all email addresses with a special token\n",
    "      no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "      no_numbers=False,               # replace all numbers with a special token\n",
    "      no_digits=True,                # replace all digits with a special token\n",
    "      no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "      no_punct=True,                 # remove punctuations\n",
    "      replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "      replace_with_url=\"\",\n",
    "      replace_with_email=\"\",\n",
    "      replace_with_phone_number=\"\",\n",
    "      replace_with_number=\"\",\n",
    "      replace_with_digit=\"0\",\n",
    "      replace_with_currency_symbol=\"\",\n",
    "      lang=\"en\" )\n",
    "  return clean_sent\n",
    "\n",
    "data_pre[\"text\"]=data_pre[\"text\"].apply(lambda x:clean_text(x))\n",
    "data_pre[\"text\"]=data_pre[\"text\"].apply(lambda x:preprocess_txt(x))\n",
    "data_pre[\"text\"]=data_pre[\"text\"].apply(lambda x:remove_stop_words(x))\n",
    "data_pre[\"text\"]=data_pre[\"text\"].apply(lambda x:get_stemmed_text(x))\n",
    "\n",
    "#############################Tokenization \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "text = data_token\n",
    "data_token_re=data_token.copy()\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "MAX_NB_WORDS=500000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data_token_re)\n",
    "data_seq = tokenizer.texts_to_sequences(data_token_re)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 5000\n",
    "data_seq_pad = pad_sequences(data_seq, padding='pre', maxlen=maxlen)\n",
    "\n",
    "\n",
    "###############################Numeric part\n",
    "from tensorflow.keras.utils import  to_categorical\n",
    "lbl_binary=to_categorical(lbl_final_rest).astype(int)\n",
    "# 4:101:mareket, 102:110:bond info, 111:157:finRatio\n",
    "d1=dum\n",
    "d2=data[list(data.iloc[:,4:101])]\n",
    "d3=data[list(data.iloc[:,102:110])]\n",
    "d4=data[list(data.iloc[:,111:157])]\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data_meta_normal1=d1.copy()\n",
    "data_meta_normal2=d2.copy()\n",
    "data_meta_normal3=d3.copy()\n",
    "data_meta_normal4=d4.copy()\n",
    "\n",
    "\n",
    "sce=MinMaxScaler()\n",
    "for item_col in data_meta_normal2.columns:\n",
    "    data_meta_normal2[item_col]=sce.fit_transform(d2[item_col].values.reshape(-1,1))\n",
    "\n",
    "for item_col in data_meta_normal3.columns:\n",
    "    data_meta_normal3[item_col]=sce.fit_transform(d3[item_col].values.reshape(-1,1))\n",
    "    \n",
    "for item_col in data_meta_normal4.columns:\n",
    "    data_meta_normal4[item_col]=sce.fit_transform(d4[item_col].values.reshape(-1,1))\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "lbl_binary=to_categorical(lbl_final_rest).astype(int)\n",
    "\n",
    "\n",
    "# In[96]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train split mode\n",
    "trian_Percentage=0.80;\n",
    "\n",
    "x_train, x_test, lbl_train, lbl_test=train_test_split(data_seq_pad, \n",
    "                                                      lbl_binary,\n",
    "                                                      test_size=1-trian_Percentage,random_state=0)\n",
    "x_train_txt, x_test_txt, lbl_train, lbl_test=train_test_split(data_token, \n",
    "                                                      lbl_binary,\n",
    "                                                      test_size=1-trian_Percentage,random_state=0)\n",
    "x_train_meta1, x_test_meta1, lbl_train, lbl_test=train_test_split(data_meta_normal1.values, \n",
    "                                                      lbl_binary,\n",
    "                                                      test_size=1-trian_Percentage,random_state=0)\n",
    "x_train_meta2, x_test_meta2, lbl_train, lbl_test=train_test_split(data_meta_normal2.values, \n",
    "                                                      lbl_binary,\n",
    "                                                      test_size=1-trian_Percentage,random_state=0)\n",
    "x_train_meta3, x_test_meta3, lbl_train, lbl_test=train_test_split(data_meta_normal3.values, \n",
    "                                                      lbl_binary,\n",
    "                                                      test_size=1-trian_Percentage,random_state=0)\n",
    "x_train_meta4, x_test_meta4, lbl_train, lbl_test=train_test_split(data_meta_normal4.values, \n",
    "                                                      lbl_binary,\n",
    "                                                      test_size=1-trian_Percentage,random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train_meta_re1=x_train_meta1.reshape(x_train_meta1.shape[0],x_train_meta1.shape[1],1)\n",
    "x_test_meta_re1=x_test_meta1.reshape(x_test_meta1.shape[0],x_test_meta1.shape[1],1)\n",
    "\n",
    "x_train_meta_re2=x_train_meta2.reshape(x_train_meta2.shape[0],x_train_meta2.shape[1],1)\n",
    "x_test_meta_re2=x_test_meta2.reshape(x_test_meta2.shape[0],x_test_meta2.shape[1],1)\n",
    "\n",
    "x_train_meta_re3=x_train_meta3.reshape(x_train_meta3.shape[0],x_train_meta3.shape[1],1)\n",
    "x_test_meta_re3=x_test_meta3.reshape(x_test_meta3.shape[0],x_test_meta3.shape[1],1)\n",
    "\n",
    "x_train_meta_re4=x_train_meta4.reshape(x_train_meta4.shape[0],x_train_meta4.shape[1],1)\n",
    "x_test_meta_re4=x_test_meta4.reshape(x_test_meta4.shape[0],x_test_meta4.shape[1],1)\n",
    "\n",
    "embedding_dim=500\n",
    "n_class=lbl_binary.shape[1]\n",
    "\n",
    "\n",
    "# In[98]:\n",
    "\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "class_weights_val = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                 classes=np.unique(np.argmax(lbl_test,axis=1)),\n",
    "                                                 y=np.argmax(lbl_test,axis=1))\n",
    "class_weight=dict()\n",
    "for idx,val in enumerate(class_weights_val):\n",
    "    class_weight[idx]=val\n",
    "\n",
    "class_weight\n",
    "\n",
    "\n",
    "# In[99]:\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "\n",
    "]\n",
    "##########################################CNN\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "def Create_CNN(data,nb_words,maxlen,embedding_dim,data_meta1,data_meta2,data_meta3,data_meta4):\n",
    "\n",
    "\n",
    "  # meta1 input1\n",
    "  input_meta1=keras.layers.Input(shape=data_meta1.shape[1:],dtype='float64',name=\"input_meta1\")  \n",
    "  layer_meta1=keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta1.1\")(input_meta1)\n",
    "  layer_meta1=keras.layers.MaxPool1D(pool_size=(2),name=\"meta1_maxpool1\")(layer_meta1)\n",
    "  layer_meta1=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(1),\n",
    "                                name=\"cov_meta1.2\")(layer_meta1)\n",
    "  #layer_meta1=keras.layers.GlobalAveragePooling1D(name=\"avg_meta1\")(layer_meta1)\n",
    "    \n",
    "  # meta2 input1\n",
    "  input_meta2=keras.layers.Input(shape=data_meta2.shape[1:],dtype='float64',name=\"input_meta2\")  \n",
    "  layer_meta2=keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta2.1\")(input_meta2)\n",
    "  layer_meta2=keras.layers.MaxPool1D(pool_size=(2),name=\"meta2_maxpool1\")(layer_meta2)\n",
    "  layer_meta2=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(1),\n",
    "                                name=\"cov_meta2.2\")(layer_meta2)\n",
    " # layer_meta2=keras.layers.GlobalAveragePooling1D(name=\"avg_meta2\")(layer_meta2)   \n",
    "    \n",
    "  # meta3 input1\n",
    "  input_meta3=keras.layers.Input(shape=data_meta3.shape[1:],dtype='float64',name=\"input_meta3\")  \n",
    "  layer_meta3=keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta3.1\")(input_meta3)\n",
    "  layer_meta3=keras.layers.MaxPool1D(pool_size=(2),name=\"meta3_maxpool1\")(layer_meta3)\n",
    "  layer_meta3=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(1),\n",
    "                                name=\"cov_meta3.2\")(layer_meta3)\n",
    " # layer_meta3=keras.layers.GlobalAveragePooling1D(name=\"avg_meta3\")(layer_meta3)\n",
    "    \n",
    "    \n",
    "  # meta4 input1\n",
    "  input_meta4=keras.layers.Input(shape=data_meta4.shape[1:],dtype='float64',name=\"input_meta4\")  \n",
    "  layer_meta4=keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta4.1\")(input_meta4)\n",
    "  layer_meta4=keras.layers.MaxPool1D(pool_size=(2),name=\"meta4_maxpool1\")(layer_meta4)\n",
    "  layer_meta4=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(1),\n",
    "                                name=\"cov_meta4.2\")(layer_meta4)\n",
    "  #layer_meta4=keras.layers.GlobalAveragePooling1D(name=\"avg_meta4\")(layer_meta4)\n",
    "\n",
    "\n",
    "  # txt input\n",
    "  word_input_cnn=keras.layers.Input(shape=(maxlen,),dtype='float64',name=\"txt_input_cnn\")  \n",
    "  em_layer_cnn=keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                  output_dim=embedding_dim,input_length=maxlen,\n",
    "                                  name=\"Embedd_cnn\")(word_input_cnn)\n",
    "  layer_data=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(2),\n",
    "                                name=\"cov_1\")(em_layer_cnn)\n",
    "  layer_data=keras.layers.MaxPool1D(pool_size=(2),name=\"data_maxpool0\")(layer_data)\n",
    "\n",
    "  layer_data =keras.layers.BatchNormalization()(layer_data) \n",
    "  layer_data =keras.layers.Dropout(0.1)(layer_data)\n",
    "\n",
    "  layer_data=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                name=\"cov_2\")(layer_data)\n",
    "  layer_data=keras.layers.MaxPool1D(pool_size=(2),name=\"data_maxpool1\")(layer_data)\n",
    "\n",
    "  layer_data =keras.layers.BatchNormalization()(layer_data)                              \n",
    "  layer_data =keras.layers.Dropout(0.1)(layer_data)\n",
    "  layer_data=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 strides=(2),\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                name=\"cov_4\")(layer_data)\n",
    "  #layer_data=keras.layers.GlobalAveragePooling1D(name=\"data_maxpool2\")(layer_data)\n",
    "\n",
    "\n",
    "\n",
    "  # hybrid\n",
    "  layer_last1 =keras.layers.Concatenate(axis=1)([layer_meta1,layer_meta2,layer_meta3,layer_meta4]) \n",
    "  layer_last = MultiHeadAttention(num_heads=5, key_dim=5, dropout=0.5, name = \"MHSA\")(layer_last1,layer_data)\n",
    "  layer_last=keras.layers.Flatten()(layer_last)\n",
    "\n",
    "  \n",
    "  layer_last =keras.layers.Dense(64,activation=\"relu\",\n",
    "                                 name=\"dens_2\")(layer_last)\n",
    "  layer_last =keras.layers.Dropout(0.5)(layer_last)\n",
    "  output= keras.layers.Dense(n_class, activation='softmax', name='out')(layer_last)\n",
    "  model = Model(inputs=[word_input_cnn,input_meta1,input_meta2,input_meta3,input_meta4],\n",
    "                       outputs=output)\n",
    "  opti=keras.optimizers.Adam(learning_rate=0.00001) #,clipnorm=0.1, decay=1e-3/200.0)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=opti,\n",
    "                       metrics=[METRICS])\n",
    "  return model\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "CNN_embedding=Create_CNN(data=data_seq_pad,\n",
    "                      nb_words=MAX_NB_WORDS,\n",
    "                      maxlen=maxlen,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      data_meta1=x_train_meta_re1,\n",
    "                      data_meta2=x_train_meta_re2,\n",
    "                      data_meta3=x_train_meta_re3,\n",
    "                      data_meta4=x_train_meta_re4)\n",
    "\n",
    "hist_CNN_embedding=CNN_embedding.fit([x_train,x_train_meta1,x_train_meta2,x_train_meta3,x_train_meta4],lbl_train,epochs=100,\n",
    "                               validation_data=([x_test,x_test_meta1,x_test_meta2,x_test_meta3,x_test_meta4],lbl_test),\n",
    "                               class_weight=class_weight,\n",
    "                               #callbacks=[save_best_model],\n",
    "                               shuffle=True,batch_size=100)\n",
    "\n",
    "pred_test=CNN_embedding.predict([x_test,x_test_meta1,x_test_meta2,x_test_meta3,x_test_meta4])\n",
    "lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def BootstrapR2(pred_test,lbl_test, numboot=10000):\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "    #---------------------------------\n",
    "    n = len(lbl_test)\n",
    "    roc1 = np.zeros((numboot, 1))\n",
    "    f1_score1 = np.zeros((numboot, 1))\n",
    "    accuracy1 = np.zeros((numboot, 1))\n",
    "    precision1 = np.zeros((numboot, 1))\n",
    "    recall1 = np.zeros((numboot, 1)) \n",
    "     \n",
    "    for i in range(numboot):\n",
    "        random_index1 = np.random.randint(0, lbl_pred.shape[0],size=len(lbl_test))\n",
    "        lbl_test1=lbl_test[random_index1]\n",
    "        pred_test1=pred_test[random_index1]\n",
    "\n",
    "        lbl_pred1=np.argmax(pred_test1,axis=1).astype(int)\n",
    "        lbl_real1=np.argmax(lbl_test1,axis=1).astype(int)\n",
    "\n",
    "        classfi_report=classification_report(lbl_real1, lbl_pred1,output_dict=True)\n",
    "\n",
    "        accuracy1[i]=accuracy_score(lbl_real1, lbl_pred1)\n",
    "        precision1[i]=classfi_report['macro avg']['precision'] \n",
    "        recall1[i]= classfi_report['macro avg']['recall']    \n",
    "        f1_score1[i]=classfi_report['macro avg']['f1-score']\n",
    "        roc1[i] = roc_auc_score(lbl_test1, pred_test1,multi_class='ovr',average='weighted') \n",
    "#----------------------\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "\n",
    "    lower,upper=np.quantile(roc1 - roc, [0.05, 0.95])\n",
    "    print(\"AUC\")\n",
    "    print(round(roc,3),\"\", round(lower,3))\n",
    "    print(round(roc,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(f1_score1 - f1_score, [0.05, 0.95])\n",
    "    print(\"f1_score\")\n",
    "    print(round(f1_score,3),\"\", round(lower,3))\n",
    "    print(round(f1_score,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(accuracy1 - accuracy, [0.05, 0.95])\n",
    "    print(\"accuracy\")\n",
    "    print(round(accuracy,3),\"\", round(lower,3))\n",
    "    print(round(accuracy,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(precision1 - precision, [0.05, 0.95])\n",
    "    print(\"precision\")\n",
    "    print(round(precision,3),\"\", round(lower,3))\n",
    "    print(round(precision,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(recall1 - recall, [0.05, 0.95])\n",
    "    print(\"recall\")\n",
    "    print(round(recall,3),\"\", round(lower,3))\n",
    "    print(round(recall,3),\"+\", round(upper,3))\n",
    "        \n",
    "    return roc1,f1_score1,accuracy1,precision1,recall1\n",
    "roc1,f1_score1,accuracy1,precision1,recall1=BootstrapR2(pred_test,lbl_test, numboot=10000)\n",
    "\n",
    "##########################################LSTM\n",
    "\n",
    "def Create_CNN(data,nb_words,maxlen,embedding_dim,data_meta1,data_meta2,data_meta3,data_meta4):\n",
    "\n",
    "\n",
    "  # meta1 input1\n",
    "  input_meta1=keras.layers.Input(shape=data_meta1.shape[1:],dtype='float64',name=\"input_meta1\")  \n",
    "  layer_meta1=keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta1.1\")(input_meta1)\n",
    "  layer_meta1 =keras.layers.Dropout(0.3)(layer_meta1)\n",
    "  layer_meta1= keras.layers.LSTM(64,return_sequences=True)(layer_meta1)#return_sequences=False\n",
    "    \n",
    "  # meta2 input1\n",
    "  input_meta2=keras.layers.Input(shape=data_meta2.shape[1:],dtype='float64',name=\"input_meta2\")  \n",
    "  layer_meta2=keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta2.1\")(input_meta2)\n",
    "  layer_meta2 =keras.layers.Dropout(0.3)(layer_meta2)\n",
    "  layer_meta2= keras.layers.LSTM(64,return_sequences=True)(layer_meta2)#return_sequences=False \n",
    "    \n",
    "  # meta3 input1\n",
    "  input_meta3=keras.layers.Input(shape=data_meta3.shape[1:],dtype='float64',name=\"input_meta3\")  \n",
    "  layer_meta3=keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta3.1\")(input_meta3)\n",
    "  layer_meta3 =keras.layers.Dropout(0.3)(layer_meta3)\n",
    "  layer_meta3= keras.layers.LSTM(64,return_sequences=True)(layer_meta3)#return_sequences=False\n",
    "    \n",
    "    \n",
    "  # meta4 input1\n",
    "  input_meta4=keras.layers.Input(shape=data_meta4.shape[1:],dtype='float64',name=\"input_meta4\")  \n",
    "  layer_meta4=keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta4.1\")(input_meta4)\n",
    "  layer_meta4 =keras.layers.Dropout(0.3)(layer_meta4)\n",
    "  layer_meta4= keras.layers.LSTM(64,return_sequences=True)(layer_meta4)#return_sequences=False\n",
    "\n",
    "\n",
    "  # txt input\n",
    "  word_input_cnn=keras.layers.Input(shape=(maxlen,),dtype='float64',name=\"txt_input_cnn\")  \n",
    "  em_layer_cnn=keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                  output_dim=embedding_dim,input_length=maxlen,\n",
    "                                  name=\"Embedd_cnn\")(word_input_cnn)\n",
    "  layer_data=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(2),\n",
    "                                name=\"cov_1\")(em_layer_cnn)\n",
    "  layer_data=keras.layers.MaxPool1D(pool_size=(2),name=\"data_maxpool0\")(layer_data)\n",
    "\n",
    "  layer_data =keras.layers.BatchNormalization()(layer_data) \n",
    "  layer_data =keras.layers.Dropout(0.1)(layer_data)\n",
    "\n",
    "  layer_data=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                name=\"cov_2\")(layer_data)\n",
    "  layer_data=keras.layers.MaxPool1D(pool_size=(2),name=\"data_maxpool1\")(layer_data)\n",
    "\n",
    "  layer_data =keras.layers.BatchNormalization()(layer_data)                              \n",
    "  layer_data =keras.layers.Dropout(0.1)(layer_data)\n",
    "  layer_data=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 strides=(2),\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                name=\"cov_4\")(layer_data)\n",
    "  layer_data =keras.layers.Dropout(0.3)(layer_data)\n",
    "  layer_data= keras.layers.LSTM(64,return_sequences=True)(layer_data)#return_sequences=False\n",
    "  #layer_data=keras.layers.GlobalAveragePooling1D(name=\"data_maxpool2\")(layer_data)\n",
    "\n",
    "\n",
    "\n",
    "  # hybrid\n",
    "  layer_last1 =keras.layers.Concatenate(axis=1)([layer_meta1,layer_meta2,layer_meta3,layer_meta4]) \n",
    "  layer_last = MultiHeadAttention(num_heads=1, key_dim=5, dropout=0.5, name = \"MHSA\")(layer_last1,layer_data)\n",
    "  layer_last=keras.layers.Flatten()(layer_last)\n",
    "\n",
    "  \n",
    "  layer_last =keras.layers.Dense(64,activation=\"relu\",\n",
    "                                 name=\"dens_2\")(layer_last)\n",
    "  layer_last =keras.layers.Dropout(0.5)(layer_last)\n",
    "  output= keras.layers.Dense(n_class, activation='softmax', name='out')(layer_last)\n",
    "  model = Model(inputs=[word_input_cnn,input_meta1,input_meta2,input_meta3,input_meta4],\n",
    "                       outputs=output)\n",
    "  opti=keras.optimizers.Adam(learning_rate=0.00001)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=opti,\n",
    "                       metrics=[METRICS])\n",
    "  return model\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "CNN_embedding=Create_CNN(data=data_seq_pad,\n",
    "                      nb_words=MAX_NB_WORDS,\n",
    "                      maxlen=maxlen,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      data_meta1=x_train_meta_re1,\n",
    "                      data_meta2=x_train_meta_re2,\n",
    "                      data_meta3=x_train_meta_re3,\n",
    "                      data_meta4=x_train_meta_re4)\n",
    "\n",
    "hist_CNN_embedding=CNN_embedding.fit([x_train,x_train_meta1,x_train_meta2,x_train_meta3,x_train_meta4],lbl_train,epochs=100,\n",
    "                               validation_data=([x_test,x_test_meta1,x_test_meta2,x_test_meta3,x_test_meta4],lbl_test),\n",
    "                               class_weight=class_weight,\n",
    "                               #callbacks=[save_best_model],\n",
    "                               shuffle=True,batch_size=100)\n",
    "\n",
    "pred_test=CNN_embedding.predict([x_test,x_test_meta1,x_test_meta2,x_test_meta3,x_test_meta4])\n",
    "lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def BootstrapR2(pred_test,lbl_test, numboot=10000):\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "    #---------------------------------\n",
    "    n = len(lbl_test)\n",
    "    roc1 = np.zeros((numboot, 1))\n",
    "    f1_score1 = np.zeros((numboot, 1))\n",
    "    accuracy1 = np.zeros((numboot, 1))\n",
    "    precision1 = np.zeros((numboot, 1))\n",
    "    recall1 = np.zeros((numboot, 1)) \n",
    "     \n",
    "    for i in range(numboot):\n",
    "        random_index1 = np.random.randint(0, lbl_pred.shape[0],size=len(lbl_test))\n",
    "        lbl_test1=lbl_test[random_index1]\n",
    "        pred_test1=pred_test[random_index1]\n",
    "\n",
    "        lbl_pred1=np.argmax(pred_test1,axis=1).astype(int)\n",
    "        lbl_real1=np.argmax(lbl_test1,axis=1).astype(int)\n",
    "\n",
    "        classfi_report=classification_report(lbl_real1, lbl_pred1,output_dict=True)\n",
    "\n",
    "        accuracy1[i]=accuracy_score(lbl_real1, lbl_pred1)\n",
    "        precision1[i]=classfi_report['macro avg']['precision'] \n",
    "        recall1[i]= classfi_report['macro avg']['recall']    \n",
    "        f1_score1[i]=classfi_report['macro avg']['f1-score']\n",
    "        roc1[i] = roc_auc_score(lbl_test1, pred_test1,multi_class='ovr',average='weighted') \n",
    "#----------------------\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "\n",
    "    lower,upper=np.quantile(roc1 - roc, [0.05, 0.95])\n",
    "    print(\"AUC\")\n",
    "    print(round(roc,3),\"\", round(lower,3))\n",
    "    print(round(roc,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(f1_score1 - f1_score, [0.05, 0.95])\n",
    "    print(\"f1_score\")\n",
    "    print(round(f1_score,3),\"\", round(lower,3))\n",
    "    print(round(f1_score,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(accuracy1 - accuracy, [0.05, 0.95])\n",
    "    print(\"accuracy\")\n",
    "    print(round(accuracy,3),\"\", round(lower,3))\n",
    "    print(round(accuracy,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(precision1 - precision, [0.05, 0.95])\n",
    "    print(\"precision\")\n",
    "    print(round(precision,3),\"\", round(lower,3))\n",
    "    print(round(precision,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(recall1 - recall, [0.05, 0.95])\n",
    "    print(\"recall\")\n",
    "    print(round(recall,3),\"\", round(lower,3))\n",
    "    print(round(recall,3),\"+\", round(upper,3))\n",
    "        \n",
    "    return roc1,f1_score1,accuracy1,precision1,recall1\n",
    "roc1,f1_score1,accuracy1,precision1,recall1=BootstrapR2(pred_test,lbl_test, numboot=10000)\n",
    "###############################################GRU\n",
    "\n",
    "def Create_CNN(data,nb_words,maxlen,embedding_dim,data_meta1,data_meta2,data_meta3,data_meta4):\n",
    "\n",
    "\n",
    " # meta1 input1\n",
    "  input_meta1=keras.layers.Input(shape=data_meta1.shape[1:],dtype='float64',name=\"input_meta1\")  \n",
    "  layer_meta1=keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta1.1\")(input_meta1)\n",
    "  layer_meta1 =keras.layers.Dropout(0.1)(layer_meta1)\n",
    "  layer_meta1=GRU(128, return_sequences=True)(layer_meta1) \n",
    "    \n",
    "  # meta2 input1\n",
    "  input_meta2=keras.layers.Input(shape=data_meta2.shape[1:],dtype='float64',name=\"input_meta2\")  \n",
    "  layer_meta2=keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta2.1\")(input_meta2)\n",
    "  layer_meta2 =keras.layers.Dropout(0.1)(layer_meta2)\n",
    "  layer_meta2=GRU(128, return_sequences=True)(layer_meta2) \n",
    "    \n",
    "  # meta3 input1\n",
    "  input_meta3=keras.layers.Input(shape=data_meta3.shape[1:],dtype='float64',name=\"input_meta3\")  \n",
    "  layer_meta3=keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta3.1\")(input_meta3)\n",
    "  layer_meta3 =keras.layers.Dropout(0.1)(layer_meta3)\n",
    "  layer_meta3=GRU(128, return_sequences=True)(layer_meta3) \n",
    "    \n",
    "    \n",
    "  # meta4 input1\n",
    "  input_meta4=keras.layers.Input(shape=data_meta4.shape[1:],dtype='float64',name=\"input_meta4\")  \n",
    "  layer_meta4=keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta4.1\")(input_meta4)\n",
    "  layer_meta4 =keras.layers.Dropout(0.1)(layer_meta4)\n",
    "  layer_meta4=GRU(128, return_sequences=True)(layer_meta4) \n",
    "\n",
    "  # txt input\n",
    "  word_input_cnn=keras.layers.Input(shape=(maxlen,),dtype='float64',name=\"txt_input_cnn\")  \n",
    "  em_layer_cnn=keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                  output_dim=embedding_dim,input_length=maxlen,\n",
    "                                  name=\"Embedd_cnn\")(word_input_cnn)\n",
    "  layer_data=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(2),\n",
    "                                name=\"cov_1\")(em_layer_cnn)\n",
    "  layer_data=keras.layers.MaxPool1D(pool_size=(2),name=\"data_maxpool0\")(layer_data)\n",
    "\n",
    "  layer_data =keras.layers.BatchNormalization()(layer_data) \n",
    "  layer_data =keras.layers.Dropout(0.1)(layer_data)\n",
    "\n",
    "  layer_data=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                name=\"cov_2\")(layer_data)\n",
    "  layer_data=keras.layers.MaxPool1D(pool_size=(2),name=\"data_maxpool1\")(layer_data)\n",
    "\n",
    "  layer_data =keras.layers.BatchNormalization()(layer_data)                              \n",
    "  layer_data =keras.layers.Dropout(0.1)(layer_data)\n",
    "  layer_data=GRU(128, return_sequences=True)(layer_data)\n",
    "#  layer_data=keras.layers.GlobalAveragePooling1D(name=\"data_maxpool2\")(layer_data)\n",
    "\n",
    "\n",
    "\n",
    "  # hybrid\n",
    "  layer_last1 =keras.layers.Concatenate(axis=1)([layer_meta1,layer_meta2,layer_meta3,layer_meta4]) \n",
    "  layer_last = MultiHeadAttention(num_heads=1, key_dim=5, dropout=0.5, name = \"MHSA\")(layer_last1,layer_data)\n",
    "  layer_last=keras.layers.Flatten()(layer_last)\n",
    "\n",
    "  \n",
    "  layer_last =keras.layers.Dense(64,activation=\"relu\",\n",
    "                                 name=\"dens_2\")(layer_last)\n",
    "  layer_last =keras.layers.Dropout(0.5)(layer_last)\n",
    "  output= keras.layers.Dense(n_class, activation='softmax', name='out')(layer_last)\n",
    "  model = Model(inputs=[word_input_cnn,input_meta1,input_meta2,input_meta3,input_meta4],\n",
    "                       outputs=output)\n",
    "  opti=keras.optimizers.Adam(learning_rate=0.00001)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=opti,\n",
    "                       metrics=[METRICS])\n",
    "  return model\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "CNN_embedding=Create_CNN(data=data_seq_pad,\n",
    "                      nb_words=MAX_NB_WORDS,\n",
    "                      maxlen=maxlen,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      data_meta1=x_train_meta_re1,\n",
    "                      data_meta2=x_train_meta_re2,\n",
    "                      data_meta3=x_train_meta_re3,\n",
    "                      data_meta4=x_train_meta_re4)\n",
    "\n",
    "hist_CNN_embedding=CNN_embedding.fit([x_train,x_train_meta1,x_train_meta2,x_train_meta3,x_train_meta4],lbl_train,epochs=100,\n",
    "                               validation_data=([x_test,x_test_meta1,x_test_meta2,x_test_meta3,x_test_meta4],lbl_test),\n",
    "                               class_weight=class_weight,\n",
    "                               #callbacks=[save_best_model],\n",
    "                               shuffle=True,batch_size=100)\n",
    "\n",
    "pred_test=CNN_embedding.predict([x_test,x_test_meta1,x_test_meta2,x_test_meta3,x_test_meta4])\n",
    "lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def BootstrapR2(pred_test,lbl_test, numboot=10000):\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "    #---------------------------------\n",
    "    n = len(lbl_test)\n",
    "    roc1 = np.zeros((numboot, 1))\n",
    "    f1_score1 = np.zeros((numboot, 1))\n",
    "    accuracy1 = np.zeros((numboot, 1))\n",
    "    precision1 = np.zeros((numboot, 1))\n",
    "    recall1 = np.zeros((numboot, 1)) \n",
    "     \n",
    "    for i in range(numboot):\n",
    "        random_index1 = np.random.randint(0, lbl_pred.shape[0],size=len(lbl_test))\n",
    "        lbl_test1=lbl_test[random_index1]\n",
    "        pred_test1=pred_test[random_index1]\n",
    "\n",
    "        lbl_pred1=np.argmax(pred_test1,axis=1).astype(int)\n",
    "        lbl_real1=np.argmax(lbl_test1,axis=1).astype(int)\n",
    "\n",
    "        classfi_report=classification_report(lbl_real1, lbl_pred1,output_dict=True)\n",
    "\n",
    "        accuracy1[i]=accuracy_score(lbl_real1, lbl_pred1)\n",
    "        precision1[i]=classfi_report['macro avg']['precision'] \n",
    "        recall1[i]= classfi_report['macro avg']['recall']    \n",
    "        f1_score1[i]=classfi_report['macro avg']['f1-score']\n",
    "        roc1[i] = roc_auc_score(lbl_test1, pred_test1,multi_class='ovr',average='weighted') \n",
    "#----------------------\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "\n",
    "    lower,upper=np.quantile(roc1 - roc, [0.05, 0.95])\n",
    "    print(\"AUC\")\n",
    "    print(round(roc,3),\"\", round(lower,3))\n",
    "    print(round(roc,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(f1_score1 - f1_score, [0.05, 0.95])\n",
    "    print(\"f1_score\")\n",
    "    print(round(f1_score,3),\"\", round(lower,3))\n",
    "    print(round(f1_score,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(accuracy1 - accuracy, [0.05, 0.95])\n",
    "    print(\"accuracy\")\n",
    "    print(round(accuracy,3),\"\", round(lower,3))\n",
    "    print(round(accuracy,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(precision1 - precision, [0.05, 0.95])\n",
    "    print(\"precision\")\n",
    "    print(round(precision,3),\"\", round(lower,3))\n",
    "    print(round(precision,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(recall1 - recall, [0.05, 0.95])\n",
    "    print(\"recall\")\n",
    "    print(round(recall,3),\"\", round(lower,3))\n",
    "    print(round(recall,3),\"+\", round(upper,3))\n",
    "        \n",
    "    return roc1,f1_score1,accuracy1,precision1,recall1\n",
    "roc1,f1_score1,accuracy1,precision1,recall1=BootstrapR2(pred_test,lbl_test, numboot=10000)\n",
    "#######################################BERT\n",
    "\n",
    "maxlen_bert=500\n",
    "#from bert import bert_tokenization\n",
    "from bert import tokenization\n",
    "import tensorflow_hub as hub\n",
    "with strategy.scope():\n",
    "   # BertTokenizer = bert_tokenization.FullTokenizer\n",
    "    BertTokenizer = tokenization.FullTokenizer\n",
    "\n",
    "with strategy.scope():\n",
    "    bert_layer = hub.KerasLayer(\"EXT/DDD/ce53fe6769d2ac3a260e92555120c54e1aecbea6\",trainable=False)\n",
    "\n",
    "    vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "    tokenizer = BertTokenizer(vocabulary_file, to_lower_case)\n",
    "\n",
    "    \n",
    "with strategy.scope():\n",
    "    def bert_encode(texts, tokenizer, max_len=512):\n",
    "        all_tokens = []\n",
    "        all_masks = []\n",
    "        all_segments = []\n",
    "\n",
    "        for text in texts:\n",
    "            text = tokenizer.tokenize(text)\n",
    "\n",
    "            text = text[:max_len-2]\n",
    "            input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "            pad_len = max_len - len(input_sequence)\n",
    "\n",
    "            tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "            tokens += [0] * pad_len\n",
    "            pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "            segment_ids = [0] * max_len\n",
    "\n",
    "            all_tokens.append(tokens)\n",
    "            all_masks.append(pad_masks)\n",
    "            all_segments.append(segment_ids)\n",
    "\n",
    "        return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "    import sys\n",
    "from absl import flags\n",
    "sys.argv=['preserve_unused_tokens=False']\n",
    "flags.FLAGS(sys.argv)\n",
    "with strategy.scope():\n",
    "    train_input_bert = bert_encode(x_train_txt, tokenizer, max_len=maxlen_bert)\n",
    "    test_input_bert = bert_encode(x_test_txt, tokenizer, max_len=maxlen_bert)\n",
    "\n",
    "\n",
    "    \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "def create_beart_model(bert_layer, max_len=512,data_meta1=[],data_meta2=[],data_meta3=[],data_meta4=[]):\n",
    "\n",
    "\n",
    "   # meta1 input1\n",
    "    input_meta1=keras.layers.Input(shape=data_meta1.shape[1:],dtype='float64',name=\"input_meta1\")  \n",
    "    layer_meta1=keras.layers.Conv1D(filters=64,\n",
    "                                   kernel_size=2,\n",
    "                                   activation='relu',\n",
    "                                   padding=\"same\",\n",
    "                                   strides=(1),\n",
    "                                   name=\"cov_meta1.1\")(input_meta1)\n",
    "    layer_meta1=keras.layers.MaxPool1D(pool_size=(2),name=\"meta1_maxpool1\")(layer_meta1)\n",
    "    layer_meta1 = MultiHeadAttention(num_heads=1, key_dim=5, dropout=0.5, name = \"MHSA1\")(layer_meta1,layer_meta1)\n",
    "    #layer_meta1=keras.layers.GlobalAveragePooling1D(name=\"avg_meta1\")(layer_meta1)   \n",
    "  # meta2 input1\n",
    "    input_meta2=keras.layers.Input(shape=data_meta2.shape[1:],dtype='float64',name=\"input_meta2\")  \n",
    "    layer_meta2=keras.layers.Conv1D(filters=64,\n",
    "                                   kernel_size=2,\n",
    "                                   activation='relu',\n",
    "                                   padding=\"same\",\n",
    "                                   strides=(1),\n",
    "                                   name=\"cov_meta2.1\")(input_meta2)\n",
    "    layer_meta2=keras.layers.MaxPool1D(pool_size=(2),name=\"meta2_maxpool1\")(layer_meta2)\n",
    "    layer_meta2 = MultiHeadAttention(num_heads=1, key_dim=5, dropout=0.5, name = \"MHSA2\")(layer_meta2,layer_meta2)\n",
    "    #layer_meta2=keras.layers.GlobalAveragePooling1D(name=\"avg_meta2\")(layer_meta2)       \n",
    "  # meta3 input1\n",
    "    input_meta3=keras.layers.Input(shape=data_meta3.shape[1:],dtype='float64',name=\"input_meta3\")  \n",
    "    layer_meta3=keras.layers.Conv1D(filters=64,\n",
    "                                   kernel_size=2,\n",
    "                                   activation='relu',\n",
    "                                   padding=\"same\",\n",
    "                                   strides=(1),\n",
    "                                   name=\"cov_meta3.1\")(input_meta3)\n",
    "    layer_meta3=keras.layers.MaxPool1D(pool_size=(2),name=\"meta3_maxpool1\")(layer_meta3)\n",
    "    layer_meta3 = MultiHeadAttention(num_heads=1, key_dim=5, dropout=0.5, name = \"MHSA3\")(layer_meta3,layer_meta3)\n",
    "    #layer_meta3=keras.layers.GlobalAveragePooling1D(name=\"avg_meta3\")(layer_meta3)      \n",
    "  # meta4 input1\n",
    "    input_meta4=keras.layers.Input(shape=data_meta4.shape[1:],dtype='float64',name=\"input_meta4\")  \n",
    "    layer_meta4=keras.layers.Conv1D(filters=64,\n",
    "                                   kernel_size=2,\n",
    "                                   activation='relu',\n",
    "                                   padding=\"same\",\n",
    "                                   strides=(1),\n",
    "                                   name=\"cov_meta4.1\")(input_meta4)\n",
    "    layer_meta4=keras.layers.MaxPool1D(pool_size=(2),name=\"meta4_maxpool1\")(layer_meta4)\n",
    "    layer_meta4 = MultiHeadAttention(num_heads=1, key_dim=5, dropout=0.5, name = \"MHSA4\")(layer_meta4,layer_meta4)\n",
    "    #layer_meta4=keras.layers.GlobalAveragePooling1D(name=\"avg_meta4\")(layer_meta4)\n",
    "\n",
    "\n",
    "# txt input\n",
    "\n",
    "    input_word_ids = keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    layer_bert = sequence_output[:, :, :]\n",
    "\n",
    "    #layer_bert=keras.layers.GlobalAveragePooling1D(name=\"avg1\")(layer_bert)\n",
    "\n",
    "  # hybrid\n",
    "    layer_last1 =keras.layers.Concatenate(axis=1)([layer_meta1,layer_meta2,layer_meta3,layer_meta4]) \n",
    "    layer_last = MultiHeadAttention(num_heads=5, key_dim=5, dropout=0.5, name = \"MHSA\")(layer_last1,layer_bert)\n",
    "    layer_last=keras.layers.Flatten()(layer_last) \n",
    "    layer_last =keras.layers.Dense(64,activation=\"relu\",name=\"dens_2\")(layer_last)\n",
    "    layer_last =keras.layers.Dropout(0.5)(layer_last)\n",
    "    output= keras.layers.Dense(n_class, activation='softmax', name='out')(layer_last)\n",
    "    bert_model = keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids,input_meta1,input_meta2,input_meta3,input_meta4], outputs=output)\n",
    "    bert_model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=keras.optimizers.Adam(learning_rate=0.00001),\n",
    "                       metrics=[\"accuracy\"])\n",
    "    return bert_model\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "model_bert = create_beart_model(bert_layer,               \n",
    "                                max_len=maxlen_bert, \n",
    "                                data_meta1=x_train_meta_re1,\n",
    "                                data_meta2=x_train_meta_re2,\n",
    "                                data_meta3=x_train_meta_re3, \n",
    "                                data_meta4=x_train_meta_re4)\n",
    "\n",
    "\n",
    "train_bert_new1=train_input_bert+(x_train_meta_re1,)+(x_train_meta_re2,)+(x_train_meta_re3,)+(x_train_meta_re4,)\n",
    "test_bert_new1=test_input_bert+(x_test_meta_re1,)+(x_test_meta_re2,)+(x_test_meta_re3,)+(x_test_meta_re4,)\n",
    "\n",
    "history=model_bert.fit(train_bert_new1,lbl_train,epochs=300,\n",
    "                               validation_data=(test_bert_new1,lbl_test),\n",
    "                               class_weight=class_weight,\n",
    "                               #callbacks=[save_best_model],\n",
    "                               shuffle=True,batch_size=100)\n",
    "\n",
    "pred_test=model_bert.predict(test_bert_new1)\n",
    "\n",
    "lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def BootstrapR2(pred_test,lbl_test, numboot=10000):\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "    #---------------------------------\n",
    "    n = len(lbl_test)\n",
    "    roc1 = np.zeros((numboot, 1))\n",
    "    f1_score1 = np.zeros((numboot, 1))\n",
    "    accuracy1 = np.zeros((numboot, 1))\n",
    "    precision1 = np.zeros((numboot, 1))\n",
    "    recall1 = np.zeros((numboot, 1)) \n",
    "     \n",
    "    for i in range(numboot):\n",
    "        random_index1 = np.random.randint(0, lbl_pred.shape[0],size=len(lbl_test))\n",
    "        lbl_test1=lbl_test[random_index1]\n",
    "        pred_test1=pred_test[random_index1]\n",
    "\n",
    "        lbl_pred1=np.argmax(pred_test1,axis=1).astype(int)\n",
    "        lbl_real1=np.argmax(lbl_test1,axis=1).astype(int)\n",
    "\n",
    "        classfi_report=classification_report(lbl_real1, lbl_pred1,output_dict=True)\n",
    "\n",
    "        accuracy1[i]=accuracy_score(lbl_real1, lbl_pred1)\n",
    "        precision1[i]=classfi_report['macro avg']['precision'] \n",
    "        recall1[i]= classfi_report['macro avg']['recall']    \n",
    "        f1_score1[i]=classfi_report['macro avg']['f1-score']\n",
    "        roc1[i] = roc_auc_score(lbl_test1, pred_test1,multi_class='ovr',average='weighted') \n",
    "#----------------------\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "\n",
    "    lower,upper=np.quantile(roc1 - roc, [0.05, 0.95])\n",
    "    print(\"AUC\")\n",
    "    print(round(roc,3),\"\", round(lower,3))\n",
    "    print(round(roc,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(f1_score1 - f1_score, [0.05, 0.95])\n",
    "    print(\"f1_score\")\n",
    "    print(round(f1_score,3),\"\", round(lower,3))\n",
    "    print(round(f1_score,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(accuracy1 - accuracy, [0.05, 0.95])\n",
    "    print(\"accuracy\")\n",
    "    print(round(accuracy,3),\"\", round(lower,3))\n",
    "    print(round(accuracy,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(precision1 - precision, [0.05, 0.95])\n",
    "    print(\"precision\")\n",
    "    print(round(precision,3),\"\", round(lower,3))\n",
    "    print(round(precision,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(recall1 - recall, [0.05, 0.95])\n",
    "    print(\"recall\")\n",
    "    print(round(recall,3),\"\", round(lower,3))\n",
    "    print(round(recall,3),\"+\", round(upper,3))\n",
    "        \n",
    "    return roc1,f1_score1,accuracy1,precision1,recall1\n",
    "roc1,f1_score1,accuracy1,precision1,recall1=BootstrapR2(pred_test,lbl_test, numboot=10000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
