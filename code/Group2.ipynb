{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8815d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_main=pd.read_csv(\"data.csv\")\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "# Set multiple gpus\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "\n",
    "#change of the rating classes to new classes\n",
    "import numpy as np\n",
    "\n",
    "lbl_first=data_main[\"RATING\"].values\n",
    "lbl_final=lbl_first.copy()\n",
    "\n",
    "dict_trans_calss={1:[1,2,3,4,5],\n",
    "                 10:[10,11,12],\n",
    "                  13:[13,14,15],\n",
    "                  16:[16,17,18,19,20,21,22]\n",
    "                  }\n",
    "\n",
    "\n",
    "for calss_main_idx in dict_trans_calss:\n",
    "  lst_class=dict_trans_calss[calss_main_idx]\n",
    "  for idx, item in enumerate(lst_class):\n",
    "    lbl_final = np.where(lbl_first == item, calss_main_idx, lbl_final)\n",
    "    \n",
    "lbl_final_rest=lbl_final.copy()\n",
    "uni_class=np.unique(lbl_final)\n",
    "for idx,item in enumerate(uni_class):\n",
    "    lbl_final_rest=np.where(lbl_final == item, idx, lbl_final_rest)\n",
    "unique, counts = np.unique(lbl_final_rest, return_counts=True)\n",
    "print(np.asarray((unique, counts)).T)\n",
    "data_pre=data_main.copy()\n",
    "data_token=data_pre[\"cleantext\"].values\n",
    "##########################Preprocessing\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from cleantext import clean\n",
    "\n",
    "# get lemmatized review\n",
    "def get_lemmatized_text(corpus):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "# get stemmed review\n",
    "def get_stemmed_text(corpus):  \n",
    "    stemmer = PorterStemmer()\n",
    "    out_text=[' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n",
    "    return out_text\n",
    "\n",
    "\n",
    "# remove all stopwords in english review\n",
    "def remove_stop_words(corpus):\n",
    "    english_stop_words = stopwords.words('english')\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split() \n",
    "                      if word not in english_stop_words])\n",
    "        )\n",
    "    return removed_stop_words\n",
    "\n",
    "# preprocess review\n",
    "def preprocess_txt(reviews):\n",
    "    REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    NO_SPACE = \"\"\n",
    "    SPACE = \" \"\n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, reviews.lower())]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    return reviews\n",
    "# clean review by cleantext libaray\n",
    "def clean_text(sent):\n",
    "  clean_sent=clean(sent,\n",
    "      fix_unicode=True,               # fix various unicode errors\n",
    "      to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "      lower=True,                     # lowercase text\n",
    "      no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
    "      no_urls=True,                  # replace all URLs with a special token\n",
    "      no_emails=True,                # replace all email addresses with a special token\n",
    "      no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "      no_numbers=False,               # replace all numbers with a special token\n",
    "      no_digits=True,                # replace all digits with a special token\n",
    "      no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "      no_punct=True,                 # remove punctuations\n",
    "      replace_with_punct=\"\",          # instead of removing punctuations you may replace them\n",
    "      replace_with_url=\"\",\n",
    "      replace_with_email=\"\",\n",
    "      replace_with_phone_number=\"\",\n",
    "      replace_with_number=\"\",\n",
    "      replace_with_digit=\"0\",\n",
    "      replace_with_currency_symbol=\"\",\n",
    "      lang=\"en\" )\n",
    "  return clean_sent\n",
    "\n",
    "data_pre[\"text\"]=data_pre[\"text\"].apply(lambda x:clean_text(x))\n",
    "data_pre[\"text\"]=data_pre[\"text\"].apply(lambda x:preprocess_txt(x))\n",
    "data_pre[\"text\"]=data_pre[\"text\"].apply(lambda x:remove_stop_words(x))\n",
    "data_pre[\"text\"]=data_pre[\"text\"].apply(lambda x:get_stemmed_text(x))\n",
    "\n",
    "#############################Tokenization \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "text = data_token\n",
    "data_token_re=data_token.copy()\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "MAX_NB_WORDS=500000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data_token_re)\n",
    "data_seq = tokenizer.texts_to_sequences(data_token_re)\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "maxlen = 5000\n",
    "data_seq_pad = pad_sequences(data_seq, padding='pre', maxlen=maxlen)\n",
    "\n",
    "\n",
    "###############################Numeric part\n",
    "from tensorflow.keras.utils import  to_categorical\n",
    "lbl_binary=to_categorical(lbl_final_rest).astype(int)\n",
    "\n",
    "\n",
    "cat=data[list(data.loc[:,['RATING_TYPE']])+list(data.loc[:,['pastrating']])]\n",
    "dum=pd.get_dummies(cat, columns=['RATING_TYPE','pastrating'],drop_first=True)\n",
    "# 4:101:mareket, 102:110:bond info, 111:157:finRatio\n",
    "num=data[list(data.iloc[:,4:101]) + list(data.iloc[:,102:110])+ list(data.iloc[:,111:157])]\n",
    "\n",
    "data_meta=num.drop('dater',axis=1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "data_meta_normal=data_meta.copy()\n",
    "sce=MinMaxScaler()\n",
    "for item_col in data_meta_normal.columns:\n",
    "    data_meta_normal[item_col]=sce.fit_transform(data_meta[item_col].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "data_meta_normal=pd.concat([dum,data_meta_normal],axis=1)\n",
    "\n",
    "data_meta_normal[\"lbl\"]=lbl_final_rest\n",
    "\n",
    "\n",
    "meat_normal=data_meta_normal.values[:,:-1]\n",
    "meat_normal.shape\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import  to_categorical\n",
    "lbl_binary=to_categorical(lbl_final_rest).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# train split mode\n",
    "trian_Percentage=0.80;\n",
    "\n",
    "x_train, x_test, lbl_train, lbl_test=train_test_split(data_seq_pad, \n",
    "                                                      lbl_binary,\n",
    "                                                      test_size=1-trian_Percentage,random_state=0)\n",
    "x_train_txt, x_test_txt, lbl_train, lbl_test=train_test_split(data_token, \n",
    "                                                      lbl_binary,\n",
    "                                                      test_size=1-trian_Percentage,random_state=0)\n",
    "x_train_meta, x_test_meta, lbl_train, lbl_test=train_test_split(meat_normal, \n",
    "                                                      lbl_binary,\n",
    "                                                      test_size=1-trian_Percentage,random_state=0)\n",
    "\n",
    "x_train_meta_re=x_train_meta.reshape(x_train_meta.shape[0],\n",
    "                                  x_train_meta.shape[1],1)\n",
    "x_test_meta_re=x_test_meta.reshape(x_test_meta.shape[0],\n",
    "                                x_test_meta.shape[1],1)\n",
    "\n",
    "print(\"-\"*50)\n",
    "print(\"train shape: {0}\".format(x_train.shape))\n",
    "print(\"test shape: {0}\".format(x_test.shape))\n",
    "print(\"x_train_txt shape: {0}\".format(x_train_txt.shape))\n",
    "print(\"x_train_meta shape: {0}\".format(x_train_meta_re.shape))\n",
    "print(\"-\"*50)\n",
    "\n",
    "\n",
    "\n",
    "embedding_dim=500\n",
    "n_class=lbl_binary.shape[1]\n",
    "\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "class_weights_val = class_weight.compute_class_weight(class_weight='balanced',\n",
    "                                                 classes=np.unique(np.argmax(lbl_test,axis=1)),\n",
    "                                                 y=np.argmax(lbl_test,axis=1))\n",
    "class_weight=dict()\n",
    "for idx,val in enumerate(class_weights_val):\n",
    "  class_weight[idx]=val\n",
    "\n",
    "class_weight\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.TruePositives(name='tp'),\n",
    "      keras.metrics.FalsePositives(name='fp'),\n",
    "      keras.metrics.TrueNegatives(name='tn'),\n",
    "      keras.metrics.FalseNegatives(name='fn'), \n",
    "      keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "      keras.metrics.Precision(name='precision'),\n",
    "      keras.metrics.Recall(name='recall'),\n",
    "      keras.metrics.AUC(name='auc'),\n",
    "      keras.metrics.AUC(name='prc', curve='PR'), # precision-recall curve\n",
    "\n",
    "]\n",
    "\n",
    "##############################################CNN\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "def Create_CNN(data,nb_words,maxlen,embedding_dim,data_meta):\n",
    "\n",
    "\n",
    "  # meta input\n",
    "  input_meta=keras.layers.Input(shape=data_meta.shape[1:],dtype='float64',name=\"input_meta\")  \n",
    "  layer_meta=keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta.1\")(input_meta)\n",
    "  layer_meta=keras.layers.MaxPool1D(pool_size=(2),name=\"meta_maxpool1\")(layer_meta)\n",
    "  layer_meta=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(1),\n",
    "                                name=\"cov_meta.2\")(layer_meta)\n",
    " #layer_meta=keras.layers.GlobalAveragePooling1D(name=\"avg_meta\")(layer_meta)\n",
    "  # txt input\n",
    "  word_input_cnn=keras.layers.Input(shape=(maxlen,),dtype='float64',name=\"txt_input_cnn\")  \n",
    "  em_layer_cnn=keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                  output_dim=embedding_dim,input_length=maxlen,\n",
    "                                  name=\"Embedd_cnn\")(word_input_cnn)\n",
    "  layer_data=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(2),\n",
    "                                name=\"cov_1\")(em_layer_cnn)\n",
    "  layer_data =keras.layers.BatchNormalization()(layer_data) \n",
    "  layer_data =keras.layers.Dropout(0.1)(layer_data)\n",
    "  layer_data=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                name=\"cov_2\")(layer_data)\n",
    "  layer_data=keras.layers.MaxPool1D(pool_size=(2),name=\"data_maxpool1\")(layer_data)\n",
    "  layer_data =keras.layers.BatchNormalization()(layer_data)                              \n",
    "  layer_data =keras.layers.Dropout(0.1)(layer_data)\n",
    "  layer_data=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 strides=(2),\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                name=\"cov_4\")(layer_data)\n",
    "  #layer_data=keras.layers.GlobalAveragePooling1D(name=\"data_maxpool2\")(layer_data)\n",
    "  # hybrid\n",
    "  #layer_last =keras.layers.Concatenate(axis=-1)([layer_meta,layer_data]) \n",
    "  layer_last = MultiHeadAttention(num_heads=5, key_dim=5, dropout=0.5, name = \"MHSA\")(layer_meta,layer_data)\n",
    "  layer_last=keras.layers.Flatten()(layer_last)\n",
    "\n",
    "  \n",
    "  layer_last =keras.layers.Dense(64,activation=\"relu\",\n",
    "                                 name=\"dens_2\")(layer_last)\n",
    "  layer_last =keras.layers.Dropout(0.5)(layer_last)\n",
    "  output= keras.layers.Dense(n_class, activation='softmax', name='out')(layer_last)\n",
    "  model = Model(inputs=[word_input_cnn,input_meta],\n",
    "                       outputs=output)\n",
    "  opti=keras.optimizers.Adam(learning_rate=0.000003)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=opti,\n",
    "                       metrics=[METRICS])\n",
    "  return model\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "CNN_embedding=Create_CNN(data=data_seq_pad,\n",
    "                      nb_words=MAX_NB_WORDS,\n",
    "                      maxlen=maxlen,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      data_meta=x_train_meta_re)\n",
    "\n",
    "\n",
    "# In[102]:\n",
    "\n",
    "\n",
    "#from tensorflow.keras.utils import plot_model\n",
    "\n",
    "#plot_model(CNN_embedding,show_shapes=True,show_dtype=True,)\n",
    "\n",
    "\n",
    "# In[103]:\n",
    "\n",
    "\n",
    "hist_CNN_embedding=CNN_embedding.fit([x_train,x_train_meta],lbl_train,epochs=300,\n",
    "                               validation_data=([x_test,x_test_meta],lbl_test),\n",
    "                               class_weight=class_weight,\n",
    "                               #callbacks=[save_best_model],\n",
    "                               shuffle=True,batch_size=100)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "pred_test=CNN_embedding.predict([x_test,x_test_meta])\n",
    "lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def BootstrapR2(pred_test,lbl_test, numboot=10000):\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "    #---------------------------------\n",
    "    n = len(lbl_test)\n",
    "    roc1 = np.zeros((numboot, 1))\n",
    "    f1_score1 = np.zeros((numboot, 1))\n",
    "    accuracy1 = np.zeros((numboot, 1))\n",
    "    precision1 = np.zeros((numboot, 1))\n",
    "    recall1 = np.zeros((numboot, 1)) \n",
    "     \n",
    "    for i in range(numboot):\n",
    "        random_index1 = np.random.randint(0, lbl_pred.shape[0],size=len(lbl_test))\n",
    "        lbl_test1=lbl_test[random_index1]\n",
    "        pred_test1=pred_test[random_index1]\n",
    "\n",
    "        lbl_pred1=np.argmax(pred_test1,axis=1).astype(int)\n",
    "        lbl_real1=np.argmax(lbl_test1,axis=1).astype(int)\n",
    "\n",
    "        classfi_report=classification_report(lbl_real1, lbl_pred1,output_dict=True)\n",
    "\n",
    "        accuracy1[i]=accuracy_score(lbl_real1, lbl_pred1)\n",
    "        precision1[i]=classfi_report['macro avg']['precision'] \n",
    "        recall1[i]= classfi_report['macro avg']['recall']    \n",
    "        f1_score1[i]=classfi_report['macro avg']['f1-score']\n",
    "        roc1[i] = roc_auc_score(lbl_test1, pred_test1,multi_class='ovr',average='weighted') \n",
    "#----------------------\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "\n",
    "    lower,upper=np.quantile(roc1 - roc, [0.05, 0.95])\n",
    "    print(\"AUC\")\n",
    "    print(round(roc,3),\"\", round(lower,3))\n",
    "    print(round(roc,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(f1_score1 - f1_score, [0.05, 0.95])\n",
    "    print(\"f1_score\")\n",
    "    print(round(f1_score,3),\"\", round(lower,3))\n",
    "    print(round(f1_score,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(accuracy1 - accuracy, [0.05, 0.95])\n",
    "    print(\"accuracy\")\n",
    "    print(round(accuracy,3),\"\", round(lower,3))\n",
    "    print(round(accuracy,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(precision1 - precision, [0.05, 0.95])\n",
    "    print(\"precision\")\n",
    "    print(round(precision,3),\"\", round(lower,3))\n",
    "    print(round(precision,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(recall1 - recall, [0.05, 0.95])\n",
    "    print(\"recall\")\n",
    "    print(round(recall,3),\"\", round(lower,3))\n",
    "    print(round(recall,3),\"+\", round(upper,3))\n",
    "        \n",
    "    return roc1,f1_score1,accuracy1,precision1,recall1\n",
    "roc1,f1_score1,accuracy1,precision1,recall1=BootstrapR2(pred_test,lbl_test, numboot=10000)\n",
    "#---------------------------------------------------------------------------------------------\n",
    "################################################################LSTM\n",
    "def Create_CNN(data,nb_words,maxlen,embedding_dim,data_meta):\n",
    "\n",
    "\n",
    "\n",
    "#meta input\n",
    "  input_meta=keras.layers.Input(shape=data_meta.shape[1:],dtype='float64',name=\"input_meta\")  \n",
    "  layer_meta=keras.layers.Conv1D(filters=64,\n",
    "                                kernel_size=2,\n",
    "                                activation='relu',\n",
    "                                padding=\"same\",\n",
    "                                strides=(1),\n",
    "                              name=\"cov_meta.1\")(input_meta)\n",
    "  layer_meta=keras.layers.MaxPool1D(pool_size=(2),name=\"meta_maxpool1\")(layer_meta)\n",
    "\n",
    "  layer_meta= keras.layers.LSTM(32,return_sequences=True)(layer_meta)#return_sequences=False\n",
    "\n",
    "\n",
    "  # txt input\n",
    "  word_input_cnn=keras.layers.Input(shape=(maxlen,),dtype='float64',name=\"txt_input_cnn\")  \n",
    "  em_layer_cnn=keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                  output_dim=embedding_dim,input_length=maxlen,\n",
    "                                  name=\"Embedd_cnn\")(word_input_cnn)\n",
    "  layer_data=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(2),\n",
    "                                name=\"cov_1\")(em_layer_cnn)\n",
    "  layer_data=keras.layers.MaxPool1D(pool_size=(2),name=\"maxpool1\")(layer_data)\n",
    "  layer_data =keras.layers.Dropout(0.1)(layer_data)\n",
    "  layer_data=keras.layers.Conv1D(filters=32,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                name=\"cov_2\")(layer_data)\n",
    "  layer_data =keras.layers.Dropout(0.1)(layer_data)\n",
    "\n",
    "  layer_data=keras.layers.Conv1D(filters=16,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(2),\n",
    "                                name=\"cov_3\")(layer_data)\n",
    "    \n",
    "  layer_data=keras.layers.MaxPool1D(pool_size=(2),name=\"maxpool2\")(layer_data)\n",
    "  layer_data= keras.layers.LSTM(32,return_sequences=True)(layer_data)\n",
    "  layer_data =keras.layers.Dropout(0.5)(layer_data)\n",
    "\n",
    "\n",
    "  # hybrid\n",
    "  #layer_last =keras.layers.Concatenate(axis=-1)([layer_meta,layer_data]) \n",
    "  layer_last = MultiHeadAttention(num_heads=5, key_dim=5, dropout=0.5, name = \"MHSA\")(layer_meta,layer_data)\n",
    "  layer_last=keras.layers.Flatten()(layer_last)\n",
    "  layer_last =keras.layers.Dense(32,activation=\"relu\",name=\"dens_1\")(layer_last)\n",
    "  layer_last =keras.layers.Dropout(0.5)(layer_last)\n",
    "  output= keras.layers.Dense(n_class, activation='softmax', name='out')(layer_last)\n",
    "  model = Model(inputs=[word_input_cnn,input_meta],\n",
    "                       outputs=output)\n",
    "  opti=keras.optimizers.Adam(learning_rate=0.000005)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=opti,\n",
    "                       metrics=[METRICS])\n",
    "  return model\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "CNN_embedding=Create_CNN(data=data_seq_pad,\n",
    "                      nb_words=MAX_NB_WORDS,\n",
    "                      maxlen=maxlen,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      data_meta=x_train_meta_re)\n",
    "\n",
    "\n",
    "# In[102]:\n",
    "\n",
    "\n",
    "#from tensorflow.keras.utils import plot_model\n",
    "\n",
    "#plot_model(CNN_embedding,show_shapes=True,show_dtype=True,)\n",
    "\n",
    "\n",
    "# In[103]:\n",
    "\n",
    "\n",
    "hist_CNN_embedding=CNN_embedding.fit([x_train,x_train_meta],lbl_train,epochs=400,\n",
    "                               validation_data=([x_test,x_test_meta],lbl_test),\n",
    "                               class_weight=class_weight,\n",
    "                               #callbacks=[save_best_model],\n",
    "                               shuffle=True,batch_size=100)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "pred_test=CNN_embedding.predict([x_test,x_test_meta])\n",
    "lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def BootstrapR2(pred_test,lbl_test, numboot=10000):\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "    #---------------------------------\n",
    "    n = len(lbl_test)\n",
    "    roc1 = np.zeros((numboot, 1))\n",
    "    f1_score1 = np.zeros((numboot, 1))\n",
    "    accuracy1 = np.zeros((numboot, 1))\n",
    "    precision1 = np.zeros((numboot, 1))\n",
    "    recall1 = np.zeros((numboot, 1)) \n",
    "     \n",
    "    for i in range(numboot):\n",
    "        random_index1 = np.random.randint(0, lbl_pred.shape[0],size=len(lbl_test))\n",
    "        lbl_test1=lbl_test[random_index1]\n",
    "        pred_test1=pred_test[random_index1]\n",
    "\n",
    "        lbl_pred1=np.argmax(pred_test1,axis=1).astype(int)\n",
    "        lbl_real1=np.argmax(lbl_test1,axis=1).astype(int)\n",
    "\n",
    "        classfi_report=classification_report(lbl_real1, lbl_pred1,output_dict=True)\n",
    "\n",
    "        accuracy1[i]=accuracy_score(lbl_real1, lbl_pred1)\n",
    "        precision1[i]=classfi_report['macro avg']['precision'] \n",
    "        recall1[i]= classfi_report['macro avg']['recall']    \n",
    "        f1_score1[i]=classfi_report['macro avg']['f1-score']\n",
    "        roc1[i] = roc_auc_score(lbl_test1, pred_test1,multi_class='ovr',average='weighted') \n",
    "#----------------------\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "\n",
    "    lower,upper=np.quantile(roc1 - roc, [0.05, 0.95])\n",
    "    print(\"AUC\")\n",
    "    print(round(roc,3),\"\", round(lower,3))\n",
    "    print(round(roc,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(f1_score1 - f1_score, [0.05, 0.95])\n",
    "    print(\"f1_score\")\n",
    "    print(round(f1_score,3),\"\", round(lower,3))\n",
    "    print(round(f1_score,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(accuracy1 - accuracy, [0.05, 0.95])\n",
    "    print(\"accuracy\")\n",
    "    print(round(accuracy,3),\"\", round(lower,3))\n",
    "    print(round(accuracy,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(precision1 - precision, [0.05, 0.95])\n",
    "    print(\"precision\")\n",
    "    print(round(precision,3),\"\", round(lower,3))\n",
    "    print(round(precision,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(recall1 - recall, [0.05, 0.95])\n",
    "    print(\"recall\")\n",
    "    print(round(recall,3),\"\", round(lower,3))\n",
    "    print(round(recall,3),\"+\", round(upper,3))\n",
    "        \n",
    "    return roc1,f1_score1,accuracy1,precision1,recall1\n",
    "roc1,f1_score1,accuracy1,precision1,recall1=BootstrapR2(pred_test,lbl_test, numboot=10000)\n",
    "#---------------------------------------------------------------------------------------------\n",
    "#######################################################GRU\n",
    "\n",
    "def Create_CNN(data,nb_words,maxlen,embedding_dim,data_meta):\n",
    "\n",
    "\n",
    "  # meta input\n",
    "  input_meta=keras.layers.Input(shape=data_meta.shape[1:],dtype='float64',name=\"input_meta\")  \n",
    " # layer_meta=keras.layers.Conv1D(filters=16,\n",
    "  #                              kernel_size=2,\n",
    "   #                             activation='relu',\n",
    "   #                             padding=\"same\",\n",
    "   #                             strides=(1),\n",
    "   #                           name=\"cov_meta.1\")(input_meta)\n",
    "  #layer_meta=keras.layers.MaxPool1D(pool_size=(2),name=\"meta_maxpool1\")(layer_meta)\n",
    "  #layer_meta=keras.layers.Conv1D(filters=64,\n",
    "  #                               kernel_size=2,\n",
    "  #                               activation='relu',\n",
    "  #                               padding=\"same\",\n",
    "  #                               strides=(1),\n",
    "  #                              name=\"cov_meta.2\")(layer_meta)\n",
    "  layer_meta=GRU(128, return_sequences=True)(input_meta)                             \n",
    "  #layer_meta=keras.layers.GlobalAveragePooling1D(name=\"avg_meta\")(layer_meta)\n",
    "  # txt input\n",
    "  word_input_cnn=keras.layers.Input(shape=(maxlen,),dtype='float64',name=\"txt_input_cnn\")  \n",
    "  em_layer_cnn=keras.layers.Embedding(input_dim=vocab_size, \n",
    "                                  output_dim=embedding_dim,input_length=maxlen,\n",
    "                                  name=\"Embedd_cnn\")(word_input_cnn)\n",
    "  layer_data=keras.layers.Conv1D(filters=64,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                 strides=(2),\n",
    "                                name=\"cov_1\")(em_layer_cnn)\n",
    "  layer_data =keras.layers.BatchNormalization()(layer_data) \n",
    "  layer_data =keras.layers.Dropout(0.1)(layer_data)\n",
    "  layer_data=keras.layers.Conv1D(filters=32,\n",
    "                                 kernel_size=2,\n",
    "                                 activation='relu',\n",
    "                                 padding=\"same\",\n",
    "                                name=\"cov_2\")(layer_data)\n",
    "  layer_data=keras.layers.MaxPool1D(pool_size=(2),name=\"data_maxpool1\")(layer_data)\n",
    "#  layer_data=keras.layers.Conv1D(filters=32,\n",
    "#                                 kernel_size=2,\n",
    "#                                 activation='relu',\n",
    "#                                 padding=\"same\",\n",
    "#                                 strides=(2),\n",
    "#                                name=\"cov_3\")(layer_data)\n",
    "#  layer_data =keras.layers.BatchNormalization()(layer_data)                              \n",
    "#  layer_data =keras.layers.Dropout(0.1)(layer_data)\n",
    "#  layer_data=keras.layers.Conv1D(filters=64,\n",
    "#                                 kernel_size=2,\n",
    "#                                 strides=(2),\n",
    "#                                 activation='relu',\n",
    "#                                 padding=\"same\",\n",
    "#                                name=\"cov_4\")(layer_data)\n",
    "  layer_data =keras.layers.BatchNormalization()(layer_data)                              \n",
    "  layer_data =keras.layers.Dropout(0.1)(layer_data)\n",
    "  #layer_data= keras.layers.LSTM(32)(layer_data)\n",
    "  layer_data=GRU(128, return_sequences=True)(layer_data)\n",
    "  #layer_data=keras.layers.GlobalAveragePooling1D(name=\"data_maxpool2\")(layer_data)\n",
    "  # hybrid\n",
    "  layer_last = MultiHeadAttention(num_heads=5, key_dim=5, dropout=0.5, name = \"MHSA\")(layer_meta,layer_data)\n",
    " # layer_last =keras.layers.Concatenate(axis=-1)([layer_meta,layer_data]) \n",
    "  layer_last=keras.layers.Flatten()(layer_last)\n",
    "  layer_last =keras.layers.Dense(64,activation=\"relu\",\n",
    "                                 name=\"dens_2\")(layer_last)\n",
    "  layer_last =keras.layers.Dropout(0.5)(layer_last)\n",
    "  output= keras.layers.Dense(n_class, activation='softmax', name='out')(layer_last)\n",
    "  model = Model(inputs=[word_input_cnn,input_meta],\n",
    "                       outputs=output)\n",
    "  opti=keras.optimizers.Adam(learning_rate=0.00001)\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=opti,\n",
    "                       metrics=[METRICS])\n",
    "  return model\n",
    "CNN_embedding=Create_CNN(data=data_seq_pad,\n",
    "                      nb_words=MAX_NB_WORDS,\n",
    "                      maxlen=maxlen,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      data_meta=x_train_meta_re)\n",
    "\n",
    "\n",
    "# In[101]:\n",
    "\n",
    "\n",
    "CNN_embedding=Create_CNN(data=data_seq_pad,\n",
    "                      nb_words=MAX_NB_WORDS,\n",
    "                      maxlen=maxlen,\n",
    "                      embedding_dim=embedding_dim,\n",
    "                      data_meta=x_train_meta_re)\n",
    "\n",
    "\n",
    "# In[102]:\n",
    "\n",
    "\n",
    "#from tensorflow.keras.utils import plot_model\n",
    "\n",
    "#plot_model(CNN_embedding,show_shapes=True,show_dtype=True,)\n",
    "\n",
    "\n",
    "# In[103]:\n",
    "\n",
    "\n",
    "hist_CNN_embedding=CNN_embedding.fit([x_train,x_train_meta],lbl_train,epochs=100,\n",
    "                               validation_data=([x_test,x_test_meta],lbl_test),\n",
    "                               class_weight=class_weight,\n",
    "                               #callbacks=[save_best_model],\n",
    "                               shuffle=True,batch_size=50)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "pred_test=CNN_embedding.predict([x_test,x_test_meta])\n",
    "lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "from sklearn.metrics import classification_report,accuracy_score,confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def BootstrapR2(pred_test,lbl_test, numboot=10000):\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "    #---------------------------------\n",
    "    n = len(lbl_test)\n",
    "    roc1 = np.zeros((numboot, 1))\n",
    "    f1_score1 = np.zeros((numboot, 1))\n",
    "    accuracy1 = np.zeros((numboot, 1))\n",
    "    precision1 = np.zeros((numboot, 1))\n",
    "    recall1 = np.zeros((numboot, 1)) \n",
    "     \n",
    "    for i in range(numboot):\n",
    "        random_index1 = np.random.randint(0, lbl_pred.shape[0],size=len(lbl_test))\n",
    "        lbl_test1=lbl_test[random_index1]\n",
    "        pred_test1=pred_test[random_index1]\n",
    "\n",
    "        lbl_pred1=np.argmax(pred_test1,axis=1).astype(int)\n",
    "        lbl_real1=np.argmax(lbl_test1,axis=1).astype(int)\n",
    "\n",
    "        classfi_report=classification_report(lbl_real1, lbl_pred1,output_dict=True)\n",
    "\n",
    "        accuracy1[i]=accuracy_score(lbl_real1, lbl_pred1)\n",
    "        precision1[i]=classfi_report['macro avg']['precision'] \n",
    "        recall1[i]= classfi_report['macro avg']['recall']    \n",
    "        f1_score1[i]=classfi_report['macro avg']['f1-score']\n",
    "        roc1[i] = roc_auc_score(lbl_test1, pred_test1,multi_class='ovr',average='weighted') \n",
    "#----------------------\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "\n",
    "    lower,upper=np.quantile(roc1 - roc, [0.05, 0.95])\n",
    "    print(\"AUC\")\n",
    "    print(round(roc,3),\"\", round(lower,3))\n",
    "    print(round(roc,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(f1_score1 - f1_score, [0.05, 0.95])\n",
    "    print(\"f1_score\")\n",
    "    print(round(f1_score,3),\"\", round(lower,3))\n",
    "    print(round(f1_score,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(accuracy1 - accuracy, [0.05, 0.95])\n",
    "    print(\"accuracy\")\n",
    "    print(round(accuracy,3),\"\", round(lower,3))\n",
    "    print(round(accuracy,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(precision1 - precision, [0.05, 0.95])\n",
    "    print(\"precision\")\n",
    "    print(round(precision,3),\"\", round(lower,3))\n",
    "    print(round(precision,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(recall1 - recall, [0.05, 0.95])\n",
    "    print(\"recall\")\n",
    "    print(round(recall,3),\"\", round(lower,3))\n",
    "    print(round(recall,3),\"+\", round(upper,3))\n",
    "        \n",
    "    return roc1,f1_score1,accuracy1,precision1,recall1\n",
    "roc1,f1_score1,accuracy1,precision1,recall1=BootstrapR2(pred_test,lbl_test, numboot=10000)\n",
    "\n",
    "#######################################################BERT\n",
    "from bert import tokenization\n",
    "import tensorflow_hub as hub\n",
    "with strategy.scope():\n",
    "    #BertTokenizer = bert_tokenization.FullTokenizer   \n",
    "    BertTokenizer = tokenization.FullTokenizer\n",
    "#bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=False)\n",
    "\n",
    "with strategy.scope():\n",
    "    bert_layer = hub.KerasLayer(\"EXT/DDD/ce53fe6769d2ac3a260e92555120c54e1aecbea6\",trainable=False)\n",
    "\n",
    "    vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "    to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "    tokenizer = BertTokenizer(vocabulary_file, to_lower_case)\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "\n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "\n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "\n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "\n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "\n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "import sys\n",
    "from absl import flags\n",
    "sys.argv=['preserve_unused_tokens=False']\n",
    "flags.FLAGS(sys.argv)\n",
    "\n",
    "with strategy.scope():\n",
    "    train_input_bert = bert_encode(x_train_txt, tokenizer, max_len=maxlen_bert)\n",
    "    test_input_bert = bert_encode(x_test_txt, tokenizer, max_len=maxlen_bert)\n",
    "    \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "def create_beart_model(bert_layer, max_len=512,data_meta=[]):\n",
    "    # meta input\n",
    "    input_meta=keras.layers.Input(shape=data_meta.shape[1:],dtype='float64',name=\"input_meta\")  \n",
    "    layer_meta=keras.layers.Conv1D(filters=16,\n",
    "                                  kernel_size=2,\n",
    "                                  activation='relu',\n",
    "                                  padding=\"same\",\n",
    "                                  strides=(1),\n",
    "                                name=\"cov_meta.1\")(input_meta)\n",
    "    layer_meta=keras.layers.MaxPool1D(pool_size=(2),name=\"meta_maxpool1\")(layer_meta)\n",
    "    layer_meta = MultiHeadAttention(num_heads=1, key_dim=5, dropout=0.5, name = \"MHSA\")(layer_meta,layer_meta)\n",
    "    #layer_meta=keras.layers.GlobalAveragePooling1D(name=\"avg_meta\")(layer_meta)\n",
    "\n",
    "   # txt input\n",
    "\n",
    "    input_word_ids = keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    layer_bert = sequence_output[:, :, :]\n",
    "    #layer_bert=keras.layers.GlobalAveragePooling1D(name=\"avg1\")(layer_bert)\n",
    "\n",
    "\n",
    "    # layer_last = keras.layers.Dense(200, activation='relu')(clf_output)\n",
    "    # # layer_last =keras.layers.BatchNormalization()(layer_last)\n",
    "    # layer_last =keras.layers.Dropout(0.2)(layer_last)\n",
    "\n",
    "    # hybrid\n",
    "#   layer_last =keras.layers.Concatenate(axis=-1)([layer_meta,layer_bert]) \n",
    "    layer_last = MultiHeadAttention(num_heads=1, key_dim=5, dropout=0.5, name = \"MHSA1\")(layer_meta,layer_bert)\n",
    "    layer_last=keras.layers.Flatten()(layer_last)\n",
    "    layer_last =keras.layers.Dense(100,activation=\"relu\",name=\"dens_1\")(layer_last)\n",
    "    layer_last =keras.layers.Dropout(0.5)(layer_last)\n",
    "    output = keras.layers.Dense(n_class, activation='softmax', name='out')(layer_last)\n",
    "    bert_model = keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids,input_meta],\n",
    "                        outputs=output)\n",
    "    bert_model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                       metrics=[\"accuracy\"])\n",
    "\n",
    "    return bert_model\n",
    "\n",
    "model_bert = create_beart_model(bert_layer, max_len=maxlen_bert,data_meta=x_train_meta_re)\n",
    "\n",
    "train_bert_new=train_input_bert+(x_train_meta_re,)\n",
    "test_bert_new=test_input_bert+(x_test_meta_re,)\n",
    "\n",
    "history = model_bert.fit(train_bert_new,lbl_train,\n",
    "                              validation_data=(test_bert_new,lbl_test),\n",
    "                              epochs=200,\n",
    "                              class_weight=class_weight,\n",
    "                              batch_size=100)\n",
    "\n",
    "\n",
    "pred_test=model_bert.predict(test_bert_new)\n",
    "lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "\n",
    "classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "# save to array\n",
    "accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "precision=classfi_report['macro avg']['precision'] \n",
    "recall= classfi_report['macro avg']['recall']    \n",
    "f1_score=classfi_report['macro avg']['f1-score']\n",
    "Con_matrix=confusion_matrix(lbl_real, lbl_pred)\n",
    "fig, ax = plot_confusion_matrix(conf_mat=Con_matrix,\n",
    "                                show_absolute=True,\n",
    "                                show_normed=True,\n",
    "                                colorbar=True,\n",
    "                                figsize=(n_class,n_class) )\n",
    "ax.set_title('confusion_matrix')\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "\n",
    "def BootstrapR2(pred_test,lbl_test, numboot=10000):\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "    #---------------------------------\n",
    "    n = len(lbl_test)\n",
    "    roc1 = np.zeros((numboot, 1))\n",
    "    f1_score1 = np.zeros((numboot, 1))\n",
    "    accuracy1 = np.zeros((numboot, 1))\n",
    "    precision1 = np.zeros((numboot, 1))\n",
    "    recall1 = np.zeros((numboot, 1)) \n",
    "     \n",
    "    for i in range(numboot):\n",
    "        random_index1 = np.random.randint(0, lbl_pred.shape[0],size=len(lbl_test))\n",
    "        lbl_test1=lbl_test[random_index1]\n",
    "        pred_test1=pred_test[random_index1]\n",
    "\n",
    "        lbl_pred1=np.argmax(pred_test1,axis=1).astype(int)\n",
    "        lbl_real1=np.argmax(lbl_test1,axis=1).astype(int)\n",
    "\n",
    "        classfi_report=classification_report(lbl_real1, lbl_pred1,output_dict=True)\n",
    "\n",
    "        accuracy1[i]=accuracy_score(lbl_real1, lbl_pred1)\n",
    "        precision1[i]=classfi_report['macro avg']['precision'] \n",
    "        recall1[i]= classfi_report['macro avg']['recall']    \n",
    "        f1_score1[i]=classfi_report['macro avg']['f1-score']\n",
    "        roc1[i] = roc_auc_score(lbl_test1, pred_test1,multi_class='ovr',average='weighted') \n",
    "#----------------------\n",
    "    lbl_pred=np.argmax(pred_test,axis=1).astype(int)\n",
    "    lbl_real=np.argmax(lbl_test,axis=1).astype(int)\n",
    "    classfi_report=classification_report(lbl_real, lbl_pred,output_dict=True)\n",
    "    accuracy=accuracy_score(lbl_real, lbl_pred)\n",
    "    precision=classfi_report['macro avg']['precision'] \n",
    "    recall= classfi_report['macro avg']['recall']    \n",
    "    f1_score=classfi_report['macro avg']['f1-score']\n",
    "    roc = roc_auc_score(lbl_test, pred_test,multi_class='ovr',average='weighted') \n",
    "\n",
    "\n",
    "    lower,upper=np.quantile(roc1 - roc, [0.05, 0.95])\n",
    "    print(\"AUC\")\n",
    "    print(round(roc,3),\"\", round(lower,3))\n",
    "    print(round(roc,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(f1_score1 - f1_score, [0.05, 0.95])\n",
    "    print(\"f1_score\")\n",
    "    print(round(f1_score,3),\"\", round(lower,3))\n",
    "    print(round(f1_score,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(accuracy1 - accuracy, [0.05, 0.95])\n",
    "    print(\"accuracy\")\n",
    "    print(round(accuracy,3),\"\", round(lower,3))\n",
    "    print(round(accuracy,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(precision1 - precision, [0.05, 0.95])\n",
    "    print(\"precision\")\n",
    "    print(round(precision,3),\"\", round(lower,3))\n",
    "    print(round(precision,3),\"+\", round(upper,3))\n",
    "\n",
    "    lower,upper=np.quantile(recall1 - recall, [0.05, 0.95])\n",
    "    print(\"recall\")\n",
    "    print(round(recall,3),\"\", round(lower,3))\n",
    "    print(round(recall,3),\"+\", round(upper,3))\n",
    "        \n",
    "    return roc1,f1_score1,accuracy1,precision1,recall1\n",
    "\n",
    "roc1,f1_score1,accuracy1,precision1,recall1=BootstrapR2(pred_test,lbl_test, numboot=10000)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
